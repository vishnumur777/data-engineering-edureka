{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6219be39-8346-420a-989a-39d215dd49a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc64dd54-35fd-4b37-bf92-de7f3e7b369a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/21 18:06:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/21 18:06:31 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/06/21 18:06:31 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/06/21 18:06:31 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[5]\").appName(\"varunSpark\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1653c73-b63b-43e1-9bb0-a05d4556c2dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-15-128.ap-south-1.compute.internal:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[5]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>varunSpark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4c35403eb0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffa30802-fb69-40e4-a58b-fd23b3c84b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.table(\"varundb.financial_trades\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bace3c01-d7fe-4eeb-a249-bc557b6c9cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- trades_id: integer (nullable = true)\n",
      " |-- timestamps: timestamp (nullable = true)\n",
      " |-- instrument: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- buyer_id: string (nullable = true)\n",
      " |-- seller_id: string (nullable = true)\n",
      " |-- trade_dates: date (nullable = true)\n",
      " |-- instruments: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e1c7be8-6be7-46c6-8b81-59a26ab87a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/21 18:06:41 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "25/06/21 18:07:11 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "25/06/21 18:07:13 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "25/06/21 18:07:17 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------+-----+--------+-----------+---------+-----------+-----------+\n",
      "| trades_id|          timestamps|instrument|price|quantity|   buyer_id|seller_id|trade_dates|instruments|\n",
      "+----------+--------------------+----------+-----+--------+-----------+---------+-----------+-----------+\n",
      "|1643953817|1970-01-01 00:09:...|       701| null|    null|2022_T00457|     AAPL| 2022-02-04|       AAPL|\n",
      "|1734078295|1970-01-01 00:04:...|       912| null|    null|2024_T00378|     MSFT| 2024-12-13|       MSFT|\n",
      "|1614633342|1970-01-01 00:17:...|       448| null|    null|2021_T00324|     TSLA| 2021-03-01|       TSLA|\n",
      "|1698738995|1970-01-01 00:16:...|       708| null|    null|2023_T00247|     MSFT| 2023-10-31|       MSFT|\n",
      "|1663919871|1970-01-01 00:23:...|       339| null|    null|2022_T00286|     MSFT| 2022-09-23|       MSFT|\n",
      "+----------+--------------------+----------+-----+--------+-----------+---------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6e98a1d-1dba-4130-83bf-ab119f319ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_metrics = (df.withColumn(\"date\", col(\"timestamps\").cast(\"date\")).groupBy(\"date\", \"instrument\").agg(sum(col(\"price\") * col(\"quantity\")) / sum(col(\"quantity\")).alias(\"VWAP\"),sum(col(\"quantity\")).alias(\"daily_volume\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60cafffd-a925-4b82-a472-94a4b2a4f1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/21 18:11:55 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "[Stage 3:====================================================>(3210 + 2) / 3212]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------------------------------------------------+------------+\n",
      "|      date|instrument|(sum((price * quantity)) / sum(quantity) AS `VWAP`)|daily_volume|\n",
      "+----------+----------+---------------------------------------------------+------------+\n",
      "|1970-01-01|       607|                                               null|        null|\n",
      "|1970-01-01|       402|                                               null|        null|\n",
      "|1970-01-01|       263|                                               null|        null|\n",
      "|1970-01-01|       114|                                               null|        null|\n",
      "|1970-01-01|       667|                                               null|        null|\n",
      "+----------+----------+---------------------------------------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "daily_metrics.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a47914b-4f0e-401c-90bc-98415451ef49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark 3",
   "language": "python",
   "name": "pyspark3_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
