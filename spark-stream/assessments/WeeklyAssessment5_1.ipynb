{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e6eca56-7b03-46ed-bae5-3912ea548c1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40201a5b-465f-46ab-b5fe-11b94447fd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f929b8e-cdc2-46f8-be85-09a2523deaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/anaconda3/lib/python3.9/site-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/varunm15t38hedu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/varunm15t38hedu/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-54f56c4c-3b93-49bc-880a-94799b90df30;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.6.0 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.8-1 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 978ms :: artifacts dl 81ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.8-1 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.6.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-54f56c4c-3b93-49bc-880a-94799b90df30\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/49ms)\n",
      "25/06/25 06:57:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:57:29 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/06/25 06:57:29 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/06/25 06:57:29 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/06/25 06:57:29 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "25/06/25 06:57:29 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "25/06/25 06:57:29 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "25/06/25 06:57:29 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "25/06/25 06:57:29 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.\n",
      "25/06/25 06:57:29 WARN Utils: Service 'SparkUI' could not bind on port 4048. Attempting port 4049.\n",
      "25/06/25 06:57:29 WARN Utils: Service 'SparkUI' could not bind on port 4049. Attempting port 4050.\n",
      "25/06/25 06:57:29 WARN Utils: Service 'SparkUI' could not bind on port 4050. Attempting port 4051.\n",
      "25/06/25 06:57:29 WARN Utils: Service 'SparkUI' could not bind on port 4051. Attempting port 4052.\n",
      "25/06/25 06:57:29 WARN Utils: Service 'SparkUI' could not bind on port 4052. Attempting port 4053.\n",
      "25/06/25 06:57:29 WARN Utils: Service 'SparkUI' could not bind on port 4053. Attempting port 4054.\n",
      "25/06/25 06:57:29 WARN Utils: Service 'SparkUI' could not bind on port 4054. Attempting port 4055.\n",
      "25/06/25 06:57:29 WARN Utils: Service 'SparkUI' could not bind on port 4055. Attempting port 4056.\n",
      "25/06/25 06:57:29 WARN Utils: Service 'SparkUI' could not bind on port 4056. Attempting port 4057.\n",
      "25/06/25 06:57:29 WARN Utils: Service 'SparkUI' could not bind on port 4057. Attempting port 4058.\n",
      "25/06/25 06:57:29 WARN Utils: Service 'SparkUI' could not bind on port 4058. Attempting port 4059.\n",
      "25/06/25 06:57:29 WARN Utils: Service 'SparkUI' could not bind on port 4059. Attempting port 4060.\n",
      "25/06/25 06:57:29 WARN Utils: Service 'SparkUI' could not bind on port 4060. Attempting port 4061.\n",
      "25/06/25 06:57:29 WARN Utils: Service 'SparkUI' could not bind on port 4061. Attempting port 4062.\n",
      "25/06/25 06:57:29 WARN Utils: Service 'SparkUI' could not bind on port 4062. Attempting port 4063.\n",
      "25/06/25 06:57:29 WARN Utils: Service 'SparkUI' could not bind on port 4063. Attempting port 4064.\n",
      "25/06/25 06:57:29 WARN Utils: Service 'SparkUI' could not bind on port 4064. Attempting port 4065.\n",
      "25/06/25 06:57:29 WARN Utils: Service 'SparkUI' could not bind on port 4065. Attempting port 4066.\n",
      "25/06/25 06:57:29 WARN Utils: Service 'SparkUI' could not bind on port 4066. Attempting port 4067.\n",
      "25/06/25 06:57:29 WARN Utils: Service 'SparkUI' could not bind on port 4067. Attempting port 4068.\n",
      "25/06/25 06:57:29 WARN Utils: Service 'SparkUI' could not bind on port 4068. Attempting port 4069.\n",
      "25/06/25 06:57:29 WARN Utils: Service 'SparkUI' could not bind on port 4069. Attempting port 4070.\n",
      "25/06/25 06:57:29 WARN Utils: Service 'SparkUI' could not bind on port 4070. Attempting port 4071.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[5]\").appName(\"VarunAssessment5\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8e82c30-fe5b-4308-951f-cd8660f586fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "116fe92d-b658-4d6b-9554-b48f21933eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor = spark.readStream.format(\"kafka\").option('kafka.bootstrap.servers', 'master:9092').option('subscribe', 'varun_muralidhar').load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83bb3e64-0972-4674-bf06-21374e27b3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sensor.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .withColumn(\"timestamp\", split(col(\"value\"), \",\")[0]) \\\n",
    "    .withColumn(\"sensor\", split(col(\"value\"), \",\")[1].cast(\"string\")) \\\n",
    "    .withColumn(\"reading\", split(col(\"value\"), \",\")[2].cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9abeddac-897c-44e6-9e3e-8d137cf8d4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_df = data.groupBy(window(\"timestamp\", \"1 minute\"), col(\"sensor\")) \\\n",
    "             .agg(avg(\"reading\").alias(\"average_reading\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf349ab9-995b-48c7-9dbd-1526b6c3e086",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/20 05:48:11 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-5364b363-4b9f-46f4-ae13-6fa6d76759a3. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/06/20 05:50:55 WARN DataStreamer: Slow waitForAckedSeqno took 41135ms (threshold=30000ms). File being written: /tmp/temporary-5364b363-4b9f-46f4-ae13-6fa6d76759a3/state/0/0/_metadata/.schema.7d4fb645-9e08-48c2-bdfc-1a6b0c6ab35b.TID12.tmp, block: BP-1364978691-172.31.64.10-1677754292516:blk_1079746168_6009401, Write pipeline datanodes: [DatanodeInfoWithStorage[172.31.64.11:9866,DS-8af7bd92-c7a4-44fb-b361-00496b8456c3,DISK], DatanodeInfoWithStorage[172.31.64.12:9866,DS-c7b06f49-e29e-4fd7-8923-882da8ea1f7c,DISK], DatanodeInfoWithStorage[172.31.64.10:9866,DS-c5d3f703-fc05-45d5-9749-75d35b7c1674,DISK]].\n",
      "25/06/20 05:52:04 WARN DataStreamer: Slow waitForAckedSeqno took 37955ms (threshold=30000ms). File being written: /tmp/temporary-5364b363-4b9f-46f4-ae13-6fa6d76759a3/state/0/38/.1.delta.e7b29c3c-1ec3-4e6b-95eb-c9d6eedc1dc8.TID49.tmp, block: BP-1364978691-172.31.64.10-1677754292516:blk_1079746430_6009663, Write pipeline datanodes: [DatanodeInfoWithStorage[172.31.73.191:9866,DS-e2b7285d-a9a5-47f8-bcee-e91f66a4d588,DISK], DatanodeInfoWithStorage[172.31.64.12:9866,DS-c7b06f49-e29e-4fd7-8923-882da8ea1f7c,DISK], DatanodeInfoWithStorage[172.31.64.11:9866,DS-8af7bd92-c7a4-44fb-b361-00496b8456c3,DISK]].\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+--------------------+--------+------------------+\n",
      "|              window|  sensor|   average_reading|\n",
      "+--------------------+--------+------------------+\n",
      "|{2025-06-19 02:40...|sensor_A|             68.49|\n",
      "|{2025-06-19 02:40...|sensor_B|             52.43|\n",
      "|{2025-06-19 02:39...|sensor_C|             33.34|\n",
      "|{2025-06-19 02:42...|sensor_C|54.885000000000005|\n",
      "|{2025-06-19 02:41...|sensor_C|             56.53|\n",
      "|{2025-06-19 02:41...|sensor_B|              43.3|\n",
      "|{2025-06-19 02:39...|sensor_A|             31.79|\n",
      "|{2025-06-19 02:38...|sensor_B|             49.33|\n",
      "+--------------------+--------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+--------------------+--------+------------------+\n",
      "|              window|  sensor|   average_reading|\n",
      "+--------------------+--------+------------------+\n",
      "|{2025-06-19 03:13...|sensor_A|62.144999999999996|\n",
      "|{2025-06-19 03:12...|sensor_B|             38.68|\n",
      "|{2025-06-19 03:00...|sensor_B|             49.09|\n",
      "|{2025-06-19 03:07...|sensor_A|             48.66|\n",
      "|{2025-06-19 03:01...|sensor_C|             41.95|\n",
      "|{2025-06-19 03:17...|sensor_A|53.364999999999995|\n",
      "|{2025-06-19 02:47...|sensor_A|             72.26|\n",
      "|{2025-06-19 02:40...|sensor_A|             68.49|\n",
      "|{2025-06-19 03:18...|sensor_B|             49.02|\n",
      "|{2025-06-19 03:11...|sensor_C|            58.705|\n",
      "|{2025-06-19 03:16...|sensor_A|             52.31|\n",
      "|{2025-06-19 03:18...|sensor_C|             54.87|\n",
      "|{2025-06-19 02:56...|sensor_A|             44.14|\n",
      "|{2025-06-19 02:46...|sensor_A|             53.37|\n",
      "|{2025-06-19 02:52...|sensor_C|             55.78|\n",
      "|{2025-06-19 02:58...|sensor_A|             53.39|\n",
      "|{2025-06-19 02:55...|sensor_A|              44.7|\n",
      "|{2025-06-19 02:40...|sensor_B|             52.43|\n",
      "|{2025-06-19 03:08...|sensor_B|             55.66|\n",
      "|{2025-06-19 02:50...|sensor_B|             40.87|\n",
      "+--------------------+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/20 05:54:47 WARN DataStreamer: Slow waitForAckedSeqno took 33596ms (threshold=30000ms). File being written: /tmp/temporary-5364b363-4b9f-46f4-ae13-6fa6d76759a3/state/0/187/.3.delta.89f71cba-2ca8-4879-9b0c-1652683dfdc6.TID599.tmp, block: BP-1364978691-172.31.64.10-1677754292516:blk_1079750550_6013783, Write pipeline datanodes: [DatanodeInfoWithStorage[172.31.73.191:9866,DS-e2b7285d-a9a5-47f8-bcee-e91f66a4d588,DISK], DatanodeInfoWithStorage[172.31.64.14:9866,DS-f823cf84-06f8-4cdd-b6e9-29d13d0292ee,DISK], DatanodeInfoWithStorage[172.31.64.13:9866,DS-e595a26d-2fb4-4683-b8bf-ad5a6b69a54b,DISK]].\n",
      "25/06/20 05:54:48 WARN DataStreamer: Slow waitForAckedSeqno took 33061ms (threshold=30000ms). File being written: /tmp/temporary-5364b363-4b9f-46f4-ae13-6fa6d76759a3/state/0/191/.3.delta.c8ac03b3-fdf4-4f51-a497-9a2d40a632cd.TID603.tmp, block: BP-1364978691-172.31.64.10-1677754292516:blk_1079750581_6013814, Write pipeline datanodes: [DatanodeInfoWithStorage[172.31.73.191:9866,DS-e2b7285d-a9a5-47f8-bcee-e91f66a4d588,DISK], DatanodeInfoWithStorage[172.31.64.14:9866,DS-f823cf84-06f8-4cdd-b6e9-29d13d0292ee,DISK], DatanodeInfoWithStorage[172.31.64.12:9866,DS-c7b06f49-e29e-4fd7-8923-882da8ea1f7c,DISK]].\n",
      "25/06/20 05:54:48 WARN DataStreamer: Slow waitForAckedSeqno took 33161ms (threshold=30000ms). File being written: /tmp/temporary-5364b363-4b9f-46f4-ae13-6fa6d76759a3/state/0/190/.3.delta.944a961e-8637-4dba-9e8c-4151c2901812.TID602.tmp, block: BP-1364978691-172.31.64.10-1677754292516:blk_1079750577_6013810, Write pipeline datanodes: [DatanodeInfoWithStorage[172.31.64.11:9866,DS-8af7bd92-c7a4-44fb-b361-00496b8456c3,DISK], DatanodeInfoWithStorage[172.31.73.191:9866,DS-e2b7285d-a9a5-47f8-bcee-e91f66a4d588,DISK], DatanodeInfoWithStorage[172.31.64.13:9866,DS-e595a26d-2fb4-4683-b8bf-ad5a6b69a54b,DISK]].\n",
      "25/06/20 05:54:48 WARN DataStreamer: Slow waitForAckedSeqno took 33678ms (threshold=30000ms). File being written: /tmp/temporary-5364b363-4b9f-46f4-ae13-6fa6d76759a3/state/0/189/.3.delta.be07f76d-7bcc-4f39-ba79-8e44a6b72486.TID601.tmp, block: BP-1364978691-172.31.64.10-1677754292516:blk_1079750564_6013797, Write pipeline datanodes: [DatanodeInfoWithStorage[172.31.64.14:9866,DS-f823cf84-06f8-4cdd-b6e9-29d13d0292ee,DISK], DatanodeInfoWithStorage[172.31.64.13:9866,DS-e595a26d-2fb4-4683-b8bf-ad5a6b69a54b,DISK], DatanodeInfoWithStorage[172.31.73.191:9866,DS-e2b7285d-a9a5-47f8-bcee-e91f66a4d588,DISK]].\n",
      "25/06/20 05:54:48 WARN DataStreamer: Slow waitForAckedSeqno took 32720ms (threshold=30000ms). File being written: /tmp/temporary-5364b363-4b9f-46f4-ae13-6fa6d76759a3/state/0/194/.3.delta.9ef80644-d006-42ab-92cb-5dc89c105686.TID606.tmp, block: BP-1364978691-172.31.64.10-1677754292516:blk_1079750597_6013830, Write pipeline datanodes: [DatanodeInfoWithStorage[172.31.64.13:9866,DS-e595a26d-2fb4-4683-b8bf-ad5a6b69a54b,DISK], DatanodeInfoWithStorage[172.31.64.12:9866,DS-c7b06f49-e29e-4fd7-8923-882da8ea1f7c,DISK], DatanodeInfoWithStorage[172.31.73.191:9866,DS-e2b7285d-a9a5-47f8-bcee-e91f66a4d588,DISK]].\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+--------------------+--------+------------------+\n",
      "|              window|  sensor|   average_reading|\n",
      "+--------------------+--------+------------------+\n",
      "|{2025-06-19 03:13...|sensor_A|62.144999999999996|\n",
      "|{2025-06-19 03:12...|sensor_B|             38.68|\n",
      "|{2025-06-19 03:00...|sensor_B|             49.09|\n",
      "|{2025-06-19 03:07...|sensor_A|             48.66|\n",
      "|{2025-06-19 03:01...|sensor_C|             41.95|\n",
      "|{2025-06-19 03:17...|sensor_A|53.364999999999995|\n",
      "|{2025-06-19 02:47...|sensor_A|             72.26|\n",
      "|{2025-06-19 02:40...|sensor_A|             68.49|\n",
      "|{2025-06-19 03:18...|sensor_B|             49.02|\n",
      "|{2025-06-19 03:11...|sensor_C|            58.705|\n",
      "|{2025-06-19 03:16...|sensor_A|             52.31|\n",
      "|{2025-06-19 03:24...|sensor_C|             50.73|\n",
      "|{2025-06-19 03:18...|sensor_C|             54.87|\n",
      "|{2025-06-19 03:24...|sensor_A|             47.07|\n",
      "|{2025-06-19 02:56...|sensor_A|             44.14|\n",
      "|{2025-06-19 02:46...|sensor_A|             53.37|\n",
      "|{2025-06-19 02:52...|sensor_C|             55.78|\n",
      "|{2025-06-19 02:58...|sensor_A|             53.39|\n",
      "|{2025-06-19 02:55...|sensor_A|              44.7|\n",
      "|{2025-06-19 02:40...|sensor_B|             52.43|\n",
      "+--------------------+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=================================>                     (123 + 5) / 200]\r"
     ]
    }
   ],
   "source": [
    "query = avg_df.writeStream.outputMode(\"complete\").format(\"console\").start()\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0c8c34-c7c7-4166-b8e4-0f1eee058107",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark 3",
   "language": "python",
   "name": "pyspark3_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
