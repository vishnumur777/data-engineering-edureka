{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f4b9e82-4e9f-4784-93f9-c00994852b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c34f6e65-581d-462d-84d5-cdd667ad82c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/12 03:32:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/12 03:32:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/06/12 03:32:54 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/06/12 03:32:54 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/06/12 03:32:54 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"VarunTimeSeries\").master(\"local[4]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c54b3087-b435-49f5-92bc-db4195366e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-32-182.ap-south-1.compute.internal:4044\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>VarunTimeSeries</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f085da1bc70>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c935fbd2-f4fb-4e65-8abd-1ae06e18f60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('stocktimeseries', sep=',', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61df0462-dd0e-4355-9661-9aefcb5720e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+-----+-----+-------+----+\n",
      "|      Date| Open| High|  Low|Close| Volume|Name|\n",
      "+----------+-----+-----+-----+-----+-------+----+\n",
      "|2006-01-03|47.47|47.85|46.25|47.58|7582127|AMZN|\n",
      "|2006-01-04|47.48|47.73|46.69|47.25|7440914|AMZN|\n",
      "|2006-01-05|47.16| 48.2|47.11|47.65|5417258|AMZN|\n",
      "|2006-01-06|47.97|48.58|47.32|47.87|6154285|AMZN|\n",
      "|2006-01-09|46.55| 47.1| 46.4|47.08|8945056|AMZN|\n",
      "+----------+-----+-----+-----+-----+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04f4ad77-c8ff-4e2a-b5e1-6f5ffc19fee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_count = df.groupBy(df.Name).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb8c6aec-8d6a-45bb-91e4-9cf29a4d22e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:======================================>               (144 + 6) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|Name|count|\n",
      "+----+-----+\n",
      "|  GE| 3020|\n",
      "| CVX| 3020|\n",
      "| AXP| 3020|\n",
      "|AMZN| 3019|\n",
      "|AAPL| 3019|\n",
      "|AABA| 3019|\n",
      "+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "comp_count.orderBy(df.Name.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "789fb1d1-d89e-44f1-9bb2-7cdbbb5e9d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a55d0230-9850-4da3-bf17-dd9facef0932",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "680c0ce0-1276-47f2-9992-e83262e740d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|year(Date)|max(High)|\n",
      "+----------+---------+\n",
      "|      2007|   101.09|\n",
      "|      2015|   696.44|\n",
      "|      2006|     76.2|\n",
      "|      2013|   405.63|\n",
      "|      2014|   408.06|\n",
      "|      2012|   264.11|\n",
      "|      2009|   145.91|\n",
      "|      2016|   847.21|\n",
      "|      2010|   185.65|\n",
      "|      2011|   246.71|\n",
      "|      2008|   104.63|\n",
      "|      2017|  1213.41|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(year(df.Date)).max().select('year(Date)','max(High)').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f467741-2bc6-4736-8fc3-c4e0b69d9151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0f08af-572a-4ee9-8309-a73900b2ea00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f22df59-8007-4828-94cd-05801d781732",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark 3",
   "language": "python",
   "name": "pyspark3_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
